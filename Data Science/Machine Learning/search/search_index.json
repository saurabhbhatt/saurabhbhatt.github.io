{
    "docs": [
        {
            "location": "/", 
            "text": "Preface\n\n\nMachine Learning (a.k.a. ML) has become one of the most exiting technologies in our age and from small to big companies like Google, Facebook, Apple, Amazon uses machine learning research and applications for good reasons. This exiciting field (i.e. Machine learning) opens the way to new possibilities in our daily lives. Few examples are\n\n\n\n\nVoice assistance on our smartphone\n\n\nRecommending the right product for our customers\n\n\nPreventing credit card fraud\n\n\nFiltering out spams from our inboxes\n\n\nDetecting and diagnosing medical diseases... and so on \n\n\n\n\nGetting exposed to practical code examples and working through example applications of machine learning are a great way to dive into this field. Concrete examples help illustrate the broader concepts by putting the learned material directly into action. \n\n\nWe are going to use Python Programming Language and Python-based machine learning libraries. Also, We will understand the mathematical concepts behind machine learning algorithms, which is essential for using machine learning successfully.\n\n\nWe can truly say that the study of machine learning will make us better scientist, thinkers and problem solvers. Knowledge is somethig which is gained by learning and the real mastery of skills can only be achieve by practice.", 
            "title": "Preface"
        }, 
        {
            "location": "/#preface", 
            "text": "Machine Learning (a.k.a. ML) has become one of the most exiting technologies in our age and from small to big companies like Google, Facebook, Apple, Amazon uses machine learning research and applications for good reasons. This exiciting field (i.e. Machine learning) opens the way to new possibilities in our daily lives. Few examples are   Voice assistance on our smartphone  Recommending the right product for our customers  Preventing credit card fraud  Filtering out spams from our inboxes  Detecting and diagnosing medical diseases... and so on    Getting exposed to practical code examples and working through example applications of machine learning are a great way to dive into this field. Concrete examples help illustrate the broader concepts by putting the learned material directly into action.   We are going to use Python Programming Language and Python-based machine learning libraries. Also, We will understand the mathematical concepts behind machine learning algorithms, which is essential for using machine learning successfully.  We can truly say that the study of machine learning will make us better scientist, thinkers and problem solvers. Knowledge is somethig which is gained by learning and the real mastery of skills can only be achieve by practice.", 
            "title": "Preface"
        }, 
        {
            "location": "/Fundamentals of Machine Learning/Classification/", 
            "text": "Chapter 3: Classification\n\n\nWe know that the most common supervised learning tasks are \nregression\n (predicting values) and \nclassification\n (predicting classes). In regression, we can predict using various algorithms like Linear Regression, Decision Trees, Random Forest etc. In this chapter, we will figure out those algorithms which will be helpful in predicting classes. \n\n\nMNIST Dataset\n\n\nIn this chapter, we will be using MNIST dataset. \n\n\nfrom\n \nsklearn.datasets\n \nimport\n \nfetch_mldata\n\n\nmnist\n \n=\n \nfetch_mldata\n(\nMNIST original\n)\n\n\n\n\n\nTraining a Binary Classifier\n\n\nPerformance Measure\n\n\nEvaluating a classifer is often significantly trickier than evaluating a regressor. There are many performance measures are available. Let's see one by one.\n\n\nMeasuring Accuracy using Cross-Validation\n\n\nA good way to evaluate a model is by choosing the cross-validation.\n\n\n(Add box wala thing)\n\n\n# Importing SGD Classifier from the library\n\n\nfrom\n \nsklearn.linear_model\n \nimport\n \nSGDClassifier\n\n\n\n# Creating framework of model\n\n\nsgd_clf\n \n=\n \nSGDClassifier\n(\nrandom_state\n \n=\n \n42\n)\n\n\n\n# Fitting data in the model to obtain the parameters\n\n\nsgd_clf\n.\nfit\n(\nX_train\n,\n \ny_train_5\n)\n\n\n\n\n\nYes, we can see that most of the time the predictor is choosing 1. This is simply because anly about 10% of the data or images are 5s. It means if we always guess that the image is not 5. We will be true for about 90% of the time. Such data is also known as skewed dataset.\n\n\nThis is also one reason that the \naccuracy\n is generally not the preferred performance measure for classifiers, especially when we are dealing with \nskewed datasets\n (i.e., when some classes are much more frequent than others)\n\n\nConfusion Matrix\n\n\nPrecision and Recall\n\n\nPrecision / Recall Tradeoff\n\n\nThe RoC Curve\n\n\nMulticlass Classification\n\n\nError Analysis\n\n\nMultilabel Classification\n\n\nMultioutput Classification\n\n\nExercise\n\n\nTitanic Dataset\n\n\nSpam Classifier\n\n\n\n\nDownload Examples of", 
            "title": "Chapter 3: Classification"
        }, 
        {
            "location": "/Fundamentals of Machine Learning/Classification/#chapter-3-classification", 
            "text": "We know that the most common supervised learning tasks are  regression  (predicting values) and  classification  (predicting classes). In regression, we can predict using various algorithms like Linear Regression, Decision Trees, Random Forest etc. In this chapter, we will figure out those algorithms which will be helpful in predicting classes.", 
            "title": "Chapter 3: Classification"
        }, 
        {
            "location": "/Fundamentals of Machine Learning/Classification/#mnist-dataset", 
            "text": "In this chapter, we will be using MNIST dataset.   from   sklearn.datasets   import   fetch_mldata  mnist   =   fetch_mldata ( MNIST original )", 
            "title": "MNIST Dataset"
        }, 
        {
            "location": "/Fundamentals of Machine Learning/Classification/#training-a-binary-classifier", 
            "text": "", 
            "title": "Training a Binary Classifier"
        }, 
        {
            "location": "/Fundamentals of Machine Learning/Classification/#performance-measure", 
            "text": "Evaluating a classifer is often significantly trickier than evaluating a regressor. There are many performance measures are available. Let's see one by one.", 
            "title": "Performance Measure"
        }, 
        {
            "location": "/Fundamentals of Machine Learning/Classification/#measuring-accuracy-using-cross-validation", 
            "text": "A good way to evaluate a model is by choosing the cross-validation.  (Add box wala thing)  # Importing SGD Classifier from the library  from   sklearn.linear_model   import   SGDClassifier  # Creating framework of model  sgd_clf   =   SGDClassifier ( random_state   =   42 )  # Fitting data in the model to obtain the parameters  sgd_clf . fit ( X_train ,   y_train_5 )   Yes, we can see that most of the time the predictor is choosing 1. This is simply because anly about 10% of the data or images are 5s. It means if we always guess that the image is not 5. We will be true for about 90% of the time. Such data is also known as skewed dataset.  This is also one reason that the  accuracy  is generally not the preferred performance measure for classifiers, especially when we are dealing with  skewed datasets  (i.e., when some classes are much more frequent than others)", 
            "title": "Measuring Accuracy using Cross-Validation"
        }, 
        {
            "location": "/Fundamentals of Machine Learning/Classification/#confusion-matrix", 
            "text": "", 
            "title": "Confusion Matrix"
        }, 
        {
            "location": "/Fundamentals of Machine Learning/Classification/#precision-and-recall", 
            "text": "", 
            "title": "Precision and Recall"
        }, 
        {
            "location": "/Fundamentals of Machine Learning/Classification/#precision-recall-tradeoff", 
            "text": "", 
            "title": "Precision / Recall Tradeoff"
        }, 
        {
            "location": "/Fundamentals of Machine Learning/Classification/#the-roc-curve", 
            "text": "", 
            "title": "The RoC Curve"
        }, 
        {
            "location": "/Fundamentals of Machine Learning/Classification/#multiclass-classification", 
            "text": "", 
            "title": "Multiclass Classification"
        }, 
        {
            "location": "/Fundamentals of Machine Learning/Classification/#error-analysis", 
            "text": "", 
            "title": "Error Analysis"
        }, 
        {
            "location": "/Fundamentals of Machine Learning/Classification/#multilabel-classification", 
            "text": "", 
            "title": "Multilabel Classification"
        }, 
        {
            "location": "/Fundamentals of Machine Learning/Classification/#multioutput-classification", 
            "text": "", 
            "title": "Multioutput Classification"
        }, 
        {
            "location": "/Fundamentals of Machine Learning/Classification/#exercise", 
            "text": "", 
            "title": "Exercise"
        }, 
        {
            "location": "/Fundamentals of Machine Learning/Classification/#titanic-dataset", 
            "text": "", 
            "title": "Titanic Dataset"
        }, 
        {
            "location": "/Fundamentals of Machine Learning/Classification/#spam-classifier", 
            "text": "Download Examples of", 
            "title": "Spam Classifier"
        }, 
        {
            "location": "/Fundamentals of Machine Learning/Training Models/", 
            "text": "Chapter 4: Training Models\n\n\nLinear Regression\n\n\nThe Normal Equation\n\n\nComputational Complexity\n\n\nGradient Descent\n\n\nBatch Gradient Descent\n\n\nStochastic Gradient Descent\n\n\nMini-batch Gradient Descent\n\n\nPolynomial Regression\n\n\nLearning Curves\n\n\nRegularized Linear Models\n\n\nRidge Regression\n\n\nLasso Regression\n\n\nElastic Net\n\n\nEarly Stopping\n\n\nLogistic Regression\n\n\nEstimating Probabilities\n\n\nTraining and Cost Function\n\n\nDecision Boundaries\n\n\nSoftmax Regression\n\n\nExercise", 
            "title": "Chapter 4: Training Models"
        }, 
        {
            "location": "/Fundamentals of Machine Learning/Training Models/#chapter-4-training-models", 
            "text": "", 
            "title": "Chapter 4: Training Models"
        }, 
        {
            "location": "/Fundamentals of Machine Learning/Training Models/#linear-regression", 
            "text": "", 
            "title": "Linear Regression"
        }, 
        {
            "location": "/Fundamentals of Machine Learning/Training Models/#the-normal-equation", 
            "text": "", 
            "title": "The Normal Equation"
        }, 
        {
            "location": "/Fundamentals of Machine Learning/Training Models/#computational-complexity", 
            "text": "", 
            "title": "Computational Complexity"
        }, 
        {
            "location": "/Fundamentals of Machine Learning/Training Models/#gradient-descent", 
            "text": "", 
            "title": "Gradient Descent"
        }, 
        {
            "location": "/Fundamentals of Machine Learning/Training Models/#batch-gradient-descent", 
            "text": "", 
            "title": "Batch Gradient Descent"
        }, 
        {
            "location": "/Fundamentals of Machine Learning/Training Models/#stochastic-gradient-descent", 
            "text": "", 
            "title": "Stochastic Gradient Descent"
        }, 
        {
            "location": "/Fundamentals of Machine Learning/Training Models/#mini-batch-gradient-descent", 
            "text": "", 
            "title": "Mini-batch Gradient Descent"
        }, 
        {
            "location": "/Fundamentals of Machine Learning/Training Models/#polynomial-regression", 
            "text": "", 
            "title": "Polynomial Regression"
        }, 
        {
            "location": "/Fundamentals of Machine Learning/Training Models/#learning-curves", 
            "text": "", 
            "title": "Learning Curves"
        }, 
        {
            "location": "/Fundamentals of Machine Learning/Training Models/#regularized-linear-models", 
            "text": "", 
            "title": "Regularized Linear Models"
        }, 
        {
            "location": "/Fundamentals of Machine Learning/Training Models/#ridge-regression", 
            "text": "", 
            "title": "Ridge Regression"
        }, 
        {
            "location": "/Fundamentals of Machine Learning/Training Models/#lasso-regression", 
            "text": "", 
            "title": "Lasso Regression"
        }, 
        {
            "location": "/Fundamentals of Machine Learning/Training Models/#elastic-net", 
            "text": "", 
            "title": "Elastic Net"
        }, 
        {
            "location": "/Fundamentals of Machine Learning/Training Models/#early-stopping", 
            "text": "", 
            "title": "Early Stopping"
        }, 
        {
            "location": "/Fundamentals of Machine Learning/Training Models/#logistic-regression", 
            "text": "", 
            "title": "Logistic Regression"
        }, 
        {
            "location": "/Fundamentals of Machine Learning/Training Models/#estimating-probabilities", 
            "text": "", 
            "title": "Estimating Probabilities"
        }, 
        {
            "location": "/Fundamentals of Machine Learning/Training Models/#training-and-cost-function", 
            "text": "", 
            "title": "Training and Cost Function"
        }, 
        {
            "location": "/Fundamentals of Machine Learning/Training Models/#decision-boundaries", 
            "text": "", 
            "title": "Decision Boundaries"
        }, 
        {
            "location": "/Fundamentals of Machine Learning/Training Models/#softmax-regression", 
            "text": "", 
            "title": "Softmax Regression"
        }, 
        {
            "location": "/Fundamentals of Machine Learning/Training Models/#exercise", 
            "text": "", 
            "title": "Exercise"
        }, 
        {
            "location": "/Fundamentals of Machine Learning/Support Vector Machines/", 
            "text": "Chapter 5: Support Vector Machines\n\n\nLinear SVM Classification\n\n\nSoft Margin Classification\n\n\nNon-linear SVM Classification", 
            "title": "Chapter 5: Support Vector Machines"
        }, 
        {
            "location": "/Fundamentals of Machine Learning/Support Vector Machines/#chapter-5-support-vector-machines", 
            "text": "", 
            "title": "Chapter 5: Support Vector Machines"
        }, 
        {
            "location": "/Fundamentals of Machine Learning/Support Vector Machines/#linear-svm-classification", 
            "text": "", 
            "title": "Linear SVM Classification"
        }, 
        {
            "location": "/Fundamentals of Machine Learning/Support Vector Machines/#soft-margin-classification", 
            "text": "", 
            "title": "Soft Margin Classification"
        }, 
        {
            "location": "/Fundamentals of Machine Learning/Support Vector Machines/#non-linear-svm-classification", 
            "text": "", 
            "title": "Non-linear SVM Classification"
        }, 
        {
            "location": "/Reinforcement Learning/Basics/", 
            "text": "Basics of Reinforcement Learning\n\n\nA single game is one episode of this process, and is represented by a finite sequence of states, actions and rewards:\n\n\n\\[s_0 , a_0 , r_1 , s_1 , a_1 , r_2 , s_2 , \\dots , s_{n-1} , a_{n-1} , r_n , s_n\\]\nSince, this is a markov decision process, the probability of state \n\\(s_{t+1}\\)\n depends only on current state \n\\(s_t\\)\n\nand action \n\\(a_t\\)\n.\n\n\nMaximizing Future Rewards\n\n\nSuppose we are an agent, our objective will be to maximize the total reward for each game. Total reward can be represented as follows:\n\n\n\\[R = \\sum_{i=1}^{n} r_{i}\\]\nIn order to maximize the total reward, the agent should try to maximize the total reward starts at any time point \n\\(t\\)\n in the game. The total reward at time step \n\\(t\\)\n is given by \n\\(R_t\\)\n and is represented as:\n\n\n\\[R_t = \\sum_{i=t}^{n} r_{i} = r_t + r_{t+1} + \\dots + r_n\\]\nThe reason we are maximizing the future reward is that we can't change the reward which we already get in the past and that is why maximization of future rewards is the only option to maximize the total reward.\n\n\nHowever, it is harder to predict the value of the rewards as we go further into the future. In order to take this into consideration, our agent should try to maximize the total discounted future reward at time \n\\(t\\)\n instead. This is done by discounting the reward at each future time step by a factor \n\\(\\gamma\\)\n over the previous time step. \n\n\n\n\nIf \n\\(\\gamma = 0\\)\n, then our network does not consider future rewards at all, and \n\n\nif \n\\(\\gamma = 1\\)\n, then our network is completely deterministic. \n\n\n\n\nA good value for \n\\(\\gamma\\)\n is around 0.9. Factoring the equation allows us to express the total discounted future reward at a given time step recursively as the sum of the current reward and the total discounted future reward at the next time step:\n\n\n\\[\\begin{split}\nR_t \n = r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\dots + \\gamma^{n-t} r_n \\\\ \n = r_t + \\gamma (r_{t+1} + \\gamma (r_{t+2} + \\dots))  \\\\ \n = r_t + \\gamma R_{t+1}\n\\end{split}\\]\nQ-Learning\n\n\nDeep reinforcement learning utilizes a model-free reinforcement learning technique called Q-learning. Q-learning can be used to find an optimal action for any given state in a finite markov decision process. Q-learning tries to maximize the value of the Q-function which represents the maximum discounted future reward when we perform action a in state s:\n\n\n\\[Q(s_t, a_t) = max(R_{t+1})\\]\nWe can define the Q-function for a transition point \n\\((s_t , a_t , r_t , s_{t+1} )\\)\n in terms of the Q-function at the next\npoint \n\\((s_{t+1} , a_{t+1} , r_{t+1} , s_{t+2} )\\)\n similar to how we did with the total discounted future reward. This\nequation is known as the \nBellman equation\n:\n\n\n\\[Q(s_t, a_t) = r_t + \\gamma max_{a_{t+1}} Q(s_{t+1}, a_{t+1})\\]\nThe deep Q-network as a Q-function\n\n\nBalancing Exploration with Exploitation\n\n\nDeep reinforcement learning is an example of online learning, where the training and prediction steps are interspersed. \n\n\nUnlike batch learning techniques where the best predictor is generated by learning on the entire training data, a predictor trained with online learning is continuously improving as it trains on new data. That is the reason that in the initial epochs of training, a deep Q-network gives random predictions which can give rise to poor Q-learning performance. \n\n\nTo alleviate this, we can use a simple exploration method such as \n\\(\\epsilon -greedy\\)\n. In case of \n\\(\\epsilon -greedy\\)\n exploration, the agent chooses the action suggested by the network with probability \n\\(1 - \\epsilon\\)\n or an action uniformly at random otherwise. That is why this strategy is called exploration/exploitation.\n\n\nAs the number of epochs increases and the Q-function converges, it begins to return more consistent Q-values. The value of \n\\(\\epsilon\\)\n can be attenuated to account for this, so as the network begins to return more consistent predictions, the agent chooses to exploit the values returned by the network over choosing random actions. \n\n\nIn case of DeepMind, the value of \n\\(\\epsilon\\)\n decreases over time from 1 to 0.1, and in our example it decreases from 0.1 to 0.001. Thus, \n\\(\\epsilon -greedy\\)\n exploration ensures that in the beginning the system balances the unreliable predictions made from the Q-network with completely random moves to explore the state space, and then settles down to less aggressive exploration (and more aggressive exploitation) as the predictions made by the Q-network improve.\n\n\nExperience replay, or the value of experience", 
            "title": "Basics of Reinforcement Learning"
        }, 
        {
            "location": "/Reinforcement Learning/Basics/#basics-of-reinforcement-learning", 
            "text": "A single game is one episode of this process, and is represented by a finite sequence of states, actions and rewards:  \\[s_0 , a_0 , r_1 , s_1 , a_1 , r_2 , s_2 , \\dots , s_{n-1} , a_{n-1} , r_n , s_n\\] Since, this is a markov decision process, the probability of state  \\(s_{t+1}\\)  depends only on current state  \\(s_t\\) \nand action  \\(a_t\\) .", 
            "title": "Basics of Reinforcement Learning"
        }, 
        {
            "location": "/Reinforcement Learning/Basics/#maximizing-future-rewards", 
            "text": "Suppose we are an agent, our objective will be to maximize the total reward for each game. Total reward can be represented as follows:  \\[R = \\sum_{i=1}^{n} r_{i}\\] In order to maximize the total reward, the agent should try to maximize the total reward starts at any time point  \\(t\\)  in the game. The total reward at time step  \\(t\\)  is given by  \\(R_t\\)  and is represented as:  \\[R_t = \\sum_{i=t}^{n} r_{i} = r_t + r_{t+1} + \\dots + r_n\\] The reason we are maximizing the future reward is that we can't change the reward which we already get in the past and that is why maximization of future rewards is the only option to maximize the total reward.  However, it is harder to predict the value of the rewards as we go further into the future. In order to take this into consideration, our agent should try to maximize the total discounted future reward at time  \\(t\\)  instead. This is done by discounting the reward at each future time step by a factor  \\(\\gamma\\)  over the previous time step.    If  \\(\\gamma = 0\\) , then our network does not consider future rewards at all, and   if  \\(\\gamma = 1\\) , then our network is completely deterministic.    A good value for  \\(\\gamma\\)  is around 0.9. Factoring the equation allows us to express the total discounted future reward at a given time step recursively as the sum of the current reward and the total discounted future reward at the next time step:  \\[\\begin{split}\nR_t   = r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\dots + \\gamma^{n-t} r_n \\\\   = r_t + \\gamma (r_{t+1} + \\gamma (r_{t+2} + \\dots))  \\\\   = r_t + \\gamma R_{t+1}\n\\end{split}\\]", 
            "title": "Maximizing Future Rewards"
        }, 
        {
            "location": "/Reinforcement Learning/Basics/#q-learning", 
            "text": "Deep reinforcement learning utilizes a model-free reinforcement learning technique called Q-learning. Q-learning can be used to find an optimal action for any given state in a finite markov decision process. Q-learning tries to maximize the value of the Q-function which represents the maximum discounted future reward when we perform action a in state s:  \\[Q(s_t, a_t) = max(R_{t+1})\\] We can define the Q-function for a transition point  \\((s_t , a_t , r_t , s_{t+1} )\\)  in terms of the Q-function at the next\npoint  \\((s_{t+1} , a_{t+1} , r_{t+1} , s_{t+2} )\\)  similar to how we did with the total discounted future reward. This\nequation is known as the  Bellman equation :  \\[Q(s_t, a_t) = r_t + \\gamma max_{a_{t+1}} Q(s_{t+1}, a_{t+1})\\]", 
            "title": "Q-Learning"
        }, 
        {
            "location": "/Reinforcement Learning/Basics/#the-deep-q-network-as-a-q-function", 
            "text": "", 
            "title": "The deep Q-network as a Q-function"
        }, 
        {
            "location": "/Reinforcement Learning/Basics/#balancing-exploration-with-exploitation", 
            "text": "Deep reinforcement learning is an example of online learning, where the training and prediction steps are interspersed.   Unlike batch learning techniques where the best predictor is generated by learning on the entire training data, a predictor trained with online learning is continuously improving as it trains on new data. That is the reason that in the initial epochs of training, a deep Q-network gives random predictions which can give rise to poor Q-learning performance.   To alleviate this, we can use a simple exploration method such as  \\(\\epsilon -greedy\\) . In case of  \\(\\epsilon -greedy\\)  exploration, the agent chooses the action suggested by the network with probability  \\(1 - \\epsilon\\)  or an action uniformly at random otherwise. That is why this strategy is called exploration/exploitation.  As the number of epochs increases and the Q-function converges, it begins to return more consistent Q-values. The value of  \\(\\epsilon\\)  can be attenuated to account for this, so as the network begins to return more consistent predictions, the agent chooses to exploit the values returned by the network over choosing random actions.   In case of DeepMind, the value of  \\(\\epsilon\\)  decreases over time from 1 to 0.1, and in our example it decreases from 0.1 to 0.001. Thus,  \\(\\epsilon -greedy\\)  exploration ensures that in the beginning the system balances the unreliable predictions made from the Q-network with completely random moves to explore the state space, and then settles down to less aggressive exploration (and more aggressive exploitation) as the predictions made by the Q-network improve.", 
            "title": "Balancing Exploration with Exploitation"
        }, 
        {
            "location": "/Reinforcement Learning/Basics/#experience-replay-or-the-value-of-experience", 
            "text": "", 
            "title": "Experience replay, or the value of experience"
        }, 
        {
            "location": "/Time Series/Basics of Time Series/", 
            "text": "Basics of Time Series\n\n\nBasics of time series modeling includes the following:\n\n\n\n\nStationary Series\n\n\nRandom Walks\n\n\nRho - Coefficient\n\n\nDickey - Fuller test of stationarity\n\n\n\n\nStationary Series\n\n\nA time series is classified as \nStationary\n is based on three criterion:\n\n\n\n\n\n\nConstant Mean:\n Mean of the series should not be a function of time. It should be constant. Mathematically, $$ \\bar{y_t} \\ne f(t) $$ Where, \n\\(\\bar{y_t}\\)\n is the mean of the time series \n\\(y_t\\)\n. Graphically, \n \nFrom the above figure, it can be seen easily that  \n\n\n\n\n\n\nConstant Variance:\n\n\n\n\n\n\nRandom Walks\n\n\nRho - Coefficient\n\n\nDickey - Fuller test of stationarity", 
            "title": "Basics of Time Series"
        }, 
        {
            "location": "/Time Series/Basics of Time Series/#basics-of-time-series", 
            "text": "Basics of time series modeling includes the following:   Stationary Series  Random Walks  Rho - Coefficient  Dickey - Fuller test of stationarity", 
            "title": "Basics of Time Series"
        }, 
        {
            "location": "/Time Series/Basics of Time Series/#stationary-series", 
            "text": "A time series is classified as  Stationary  is based on three criterion:    Constant Mean:  Mean of the series should not be a function of time. It should be constant. Mathematically, $$ \\bar{y_t} \\ne f(t) $$ Where,  \\(\\bar{y_t}\\)  is the mean of the time series  \\(y_t\\) . Graphically,    From the above figure, it can be seen easily that      Constant Variance:", 
            "title": "Stationary Series"
        }, 
        {
            "location": "/Time Series/Basics of Time Series/#random-walks", 
            "text": "", 
            "title": "Random Walks"
        }, 
        {
            "location": "/Time Series/Basics of Time Series/#rho-coefficient", 
            "text": "", 
            "title": "Rho - Coefficient"
        }, 
        {
            "location": "/Time Series/Basics of Time Series/#dickey-fuller-test-of-stationarity", 
            "text": "", 
            "title": "Dickey - Fuller test of stationarity"
        }, 
        {
            "location": "/Recommendation Engine/Collaborative Filtering/", 
            "text": "Collaborative Filtering\n\n\nCollaborative filtering (CF) uses the rating data given by the user for many items or products. CF is used for the following two tasks:\n\n\n\n\nPredicting the missing ratings\n\n\nCreating a top N recommendation list for a given user a.k.a. the \nactive user\n.\n\n\n\n\nMathematical Notation\n\n\nGroups of Collaborative filtering algorithms\n\n\nCollaborative Filtering algorithms are typically divided into two groups:\n\n\n\n\n\n\nMemory-Based CF Algorithms\n\n\n\n\nUses the whole user database to create recommendations\n\n\nAlso the most prominent algorithm\n\n\nDisadvantage:\n The disadvantage of this approach is \nscalability\n since the whole user database has to be processed online for creating recommendations\n\n\n\n\n\n\n\n\nModel-Based CF Algorithms\n\n\n\n\nUses the user database to learn a more compact model that is later used to create recommendations\n\n\n\n\n\n\n\n\nUser-based Collaborative Filtering\n\n\nItem-based Collaborative Filtering", 
            "title": "Collaborative Filtering"
        }, 
        {
            "location": "/Recommendation Engine/Collaborative Filtering/#collaborative-filtering", 
            "text": "Collaborative filtering (CF) uses the rating data given by the user for many items or products. CF is used for the following two tasks:   Predicting the missing ratings  Creating a top N recommendation list for a given user a.k.a. the  active user .", 
            "title": "Collaborative Filtering"
        }, 
        {
            "location": "/Recommendation Engine/Collaborative Filtering/#mathematical-notation", 
            "text": "", 
            "title": "Mathematical Notation"
        }, 
        {
            "location": "/Recommendation Engine/Collaborative Filtering/#groups-of-collaborative-filtering-algorithms", 
            "text": "Collaborative Filtering algorithms are typically divided into two groups:    Memory-Based CF Algorithms   Uses the whole user database to create recommendations  Also the most prominent algorithm  Disadvantage:  The disadvantage of this approach is  scalability  since the whole user database has to be processed online for creating recommendations     Model-Based CF Algorithms   Uses the user database to learn a more compact model that is later used to create recommendations", 
            "title": "Groups of Collaborative filtering algorithms"
        }, 
        {
            "location": "/Recommendation Engine/Collaborative Filtering/#user-based-collaborative-filtering", 
            "text": "", 
            "title": "User-based Collaborative Filtering"
        }, 
        {
            "location": "/Recommendation Engine/Collaborative Filtering/#item-based-collaborative-filtering", 
            "text": "", 
            "title": "Item-based Collaborative Filtering"
        }
    ]
}